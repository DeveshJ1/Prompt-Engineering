# -*- coding: utf-8 -*-
"""addition_prompting.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XsGHyj-QROcq1bWFewW1SrIn-9IQ2oo5
"""

!pip install --upgrade together
import re
from together import Together
import numpy as np
import pandas as pd
from tqdm import tqdm
from time import sleep
from sklearn.metrics import mean_absolute_error

def debug_print(s, debug):
    if debug:
        print(s)

# TODO: find your API key here
# https://api.together.xyz/settings/api-keys

from together import Together

client = Together(api_key='d19727847af1b9525830756b7e81a558d97249098b5cb3675013c86b2e3246ef')

def call_together_api(prompt, student_configs, post_processing, model="meta-llama/Llama-2-13b-chat-hf", debug=False):
    # print(prompt)
    response = client.completions.create(
        prompt=prompt,
        model=model,
        **student_configs
    )

    debug_print("************ Prompt ************", debug)
    debug_print(prompt, debug)

    debug_print("************ Response ************", debug)
    # print(response)
    result = response.choices[0].text
    debug_print(result, debug)

    debug_print("************ Final Output ************", debug)
    final_output = post_processing(result)
    debug_print(final_output, debug)

    return final_output

"""###  Part 1. Zero Shot Addition"""

def get_addition_pairs(lower_bound, upper_bound, rng):
    int_a = int(np.ceil(rng.uniform(lower_bound, upper_bound)))
    int_b = int(np.ceil(rng.uniform(lower_bound, upper_bound)))
    int_c = int(np.ceil(rng.uniform(lower_bound, upper_bound)))
    return int_a, int_b, int_c

def test_range(added_prompt, prompt_configs, rng, n_sample=30,
               lower_bound=1, upper_bound=10, fixed_pairs=None,
               pre_processing=lambda x:x, post_processing=lambda y:y,
               model='meta-llama/Llama-2-13b-chat-hf', debug=False):
    int_as = []
    int_bs = []
    int_cs = []
    answers = []
    model_responses = []
    correct = []
    prompts = []
    iterations = range(n_sample) if fixed_pairs is None else fixed_pairs
    for i, v in enumerate(tqdm(iterations)):
        if fixed_pairs is None:
            int_a, int_b, int_c = get_addition_pairs(lower_bound=lower_bound, upper_bound=upper_bound, rng=rng)
        else:
            int_a, int_b, int_c = v
        fixed_prompt = f'{int_a}+{int_b}+{int_c}'
        fixed_prompt = pre_processing(fixed_prompt)
        prefix, suffix = added_prompt
        prompt = prefix + fixed_prompt + suffix
        model_response = call_together_api(prompt, prompt_configs, post_processing, model=model, debug=debug)
        answer = int_a + int_b + int_c
        int_as.append(int_a)
        int_bs.append(int_b)
        int_cs.append(int_c)
        prompts.append(prompt)
        answers.append(answer)
        model_responses.append(model_response)
        correct.append((answer == model_response))
        sleep(1) # pause to not trigger DDoS defense
    df = pd.DataFrame({'int_a': int_as, 'int_b': int_bs, 'int_c': int_cs, 'prompt': prompts, 'answer': answers, 'response': model_responses, 'correct': correct})
    print(df)
    mae = mean_absolute_error(df['answer'], df['response'])
    acc = df.correct.sum()/len(df)
    prompt_length = len(prefix) + len(suffix)
    res = acc * 1/prompt_length * (1-mae/(1*10^4))
    return {'res': res, 'acc': acc, 'mae': mae, 'prompt_length': prompt_length}

"""**Example: Zero-shot single-digit addition**"""

added_prompt = ('Question: What is ', '?\n Final Answer: ') # Question: What is a+b+c?\nAnswer:
prompt_config = {'max_tokens': 2,
                'temperature': 0.7,
                'top_k': 50,
                'top_p': 0.6,
                'repetition_penalty': 1,
                'stop': []}

# input_string: 'a+b+c'
def your_pre_processing(input_string):
    return input_string

# output_string:
# depending on your prompt, it might look like 'output: number'
def your_post_processing(output_string):
    # using regular expression to find the first consecutive digits in the returned string
    only_digits = re.sub(r"\D", "", output_string)
    try:
        res = int(only_digits)
    except:
        res = 0
    return res

model = 'meta-llama/Llama-2-13b-chat-hf'
print(model)
seed = 0
rng = np.random.default_rng(seed)
res = test_range(added_prompt=added_prompt, prompt_configs=prompt_config, rng=rng, n_sample=10, lower_bound=1, upper_bound=10, fixed_pairs=None, pre_processing=your_pre_processing, post_processing=your_post_processing, model=model, debug=False)
print(res)

"""**Example: Zero-shot 3 5-digit addition**"""

sleep(1) # wait a little bit to prevent api call error
prompt_config['max_tokens'] = 6
rng = np.random.default_rng(seed)
res = test_range(added_prompt=added_prompt, prompt_configs=prompt_config, rng=rng, n_sample=10, lower_bound=10000, upper_bound=99999, fixed_pairs=None, pre_processing=your_pre_processing, post_processing=your_post_processing, model=model, debug=False)
print(res)

"""-------

**Q1a.** In your opinion, what are some factors that cause language model performance to deteriorate from 1 digit to 5 digits?

Answer:

-------

**Q1b**. Play around with the config parameters ('max_tokens','temperature','top_k','top_p','repetition_penalty') in together.ai's [web UI](https://api.together.xyz/playground/chat/meta-llama/Llama-2-13b-chat-hf).
* What does each parameter represent?
* How does increasing each parameter change the generation?

Answer:

------

**Q1c**. Do 5-digit addition with 70B parameter llama model.
* How does the performance change?
* What are some factors that cause this change?

Answer:
"""

sleep(1) # wait a little bit to prevent api call error
rng = np.random.default_rng(seed)
res = test_range(added_prompt=added_prompt, prompt_configs=prompt_config, rng=rng, n_sample=10, lower_bound=10000, upper_bound=99999, fixed_pairs=None, pre_processing=your_pre_processing, post_processing=your_post_processing, model='meta-llama/Llama-2-70b-hf', debug=False)
print(res)

"""------

**Q1d.** Here we're giving our language model the prior that the sum of three 5-digit numbers must have a maximum of 6 digits. (by setting max_token=6). What if we remove this prior by increasing the max_token to 20?
* Does the model still perform well?
* What are some reasons why?

Answer:
"""

sleep(1) # wait a little bit to prevent api call error
added_prompt = ('Question: What is ', '?\nAnswer: ') # Question: What is a+b+c?\nAnswer:
prompt_config = {'max_tokens': 20,
                'temperature': 0.7,
                'top_k': 50,
                'top_p': 0.6,
                'repetition_penalty': 1,
                'stop': []}

# input_string: 'a+b'
def your_pre_processing(input_string):
    return input_string

def your_post_processing(output_string):
    first_line = output_string.splitlines()[0]
    only_digits = re.sub(r"\D", "", first_line)
    try:
        res = int(only_digits)
    except:
        res = 0
    return res


model = 'meta-llama/Llama-2-13b-chat-hf'
print(model)
seed = 0
rng = np.random.default_rng(seed)
res = test_range(added_prompt=added_prompt, prompt_configs=prompt_config, rng=rng, n_sample=10, lower_bound=10000, upper_bound=99999, fixed_pairs=None, pre_processing=your_pre_processing, post_processing=your_post_processing, model=model, debug=False)
print(res)

"""### Part 2. In Context Learning

We will try to improve the performance of 5-digit additions via in-context learning.
For cost-control purposes (you only have $25 free credits), we will use [llama-2-13b-chat](https://api.together.xyz/models/meta-llama/Llama-2-13b-chat-hf). Below is a simple example.

Let's first see how Incontext Learning helps 1 digit additions
"""

sleep(1) # wait a little bit to prevent api call error
added_prompt = ('Question: What is 3+7+5?\nAnswer: 15\n Question: What is ', '?\nAnswer: ') # Question: What is a+b?\nAnswer:
prompt_config = {'max_tokens': 6,
                'temperature': 0.7,
                'top_k': 50,
                'top_p': 0.6,
                'repetition_penalty': 1,
                'stop': []}
rng = np.random.default_rng(seed)
res = test_range(added_prompt=added_prompt, prompt_configs=prompt_config, rng=rng, n_sample=10, lower_bound=1, upper_bound=9, fixed_pairs=None, pre_processing=your_pre_processing, post_processing=your_post_processing, model='meta-llama/Llama-2-13b-chat-hf', debug=False)
print(res)

"""**Q2a**.
* How does the performance change with the baseline in-context learning prompt? (compare with "Example: Zero-shot 1-digit addition" in Q1)
* What are some factors that cause this change?

Answer:

------

In Context Learning Applied to 5 Digit Additions
"""

sleep(1) # wait a little bit to prevent api call error
added_prompt = ('Question: What is 3+7+5?\nAnswer: 15\n Question: What is ', '?\nAnswer: ') # Question: What is a+b?\nAnswer:
prompt_config = {'max_tokens': 6,
                'temperature': 0.7,
                'top_k': 50,
                'top_p': 0.6,
                'repetition_penalty': 1,
                'stop': []}
rng = np.random.default_rng(seed)
res = test_range(added_prompt=added_prompt, prompt_configs=prompt_config, rng=rng, n_sample=10, lower_bound=10000, upper_bound=99999, fixed_pairs=None, pre_processing=your_pre_processing, post_processing=your_post_processing, model='meta-llama/Llama-2-13b-chat-hf', debug=False)
print(res)

"""**Q2b**.
* How does the performance change with the baseline in-context learning prompt? (compare with "Example: Zero-shot 5-digit addition" in Q1)
* What are some factors that cause this change?

Answer:

------

Now we will remove the prior on output length and re-evaluate the performance of our baseline one-shot learning prompt. We need to modify our post processing function to extract the answer from the output sequence. In this case, it is the number in the first line that starts with "Answer: ".

**Q2c**.
* How does the performance change when we relax the output length constraint? (compare with Q2a)
* What are some factors that cause this change?

Answer:
"""

sleep(1) # wait a little bit to prevent api call error

prompt_config['max_tokens'] = 50 # changed from 10, assuming we don't know the output length Question: What is 12345+54321+67890?\nAnswer: 134556
added_propmpt=('Question: What is 3+7+5?\nAnswer: 15\n Question: What is 12345+54321+67890?\nAnswer: 134556\n Question What is 98765+54321+12345?\nAnswer: 165431\n Question: What is ', '?\nAnswer: ')
rng = np.random.default_rng(seed)
res = test_range(added_prompt=added_prompt, prompt_configs=prompt_config, rng=rng, n_sample=10, lower_bound=10000, upper_bound=99999, fixed_pairs=None, pre_processing=your_pre_processing, post_processing=your_post_processing, model='meta-llama/Llama-2-13b-chat-hf', debug=False)
print(res)

"""**Q2d.** Let's look at a specific example with large absolute error.
* Run the cell at least 5 times. Does the error change with each time? Why?
* Can you think of a prompt to reduce the error?
* Why do you think it would work?
* Does it work in practice? Why or why not?
"""

test_range(added_prompt=added_prompt, prompt_configs=prompt_config, rng=rng, fixed_pairs=[(90909,10101, 54565)], pre_processing=your_pre_processing, post_processing=your_post_processing, model='meta-llama/Llama-2-13b-chat-hf', debug=True)